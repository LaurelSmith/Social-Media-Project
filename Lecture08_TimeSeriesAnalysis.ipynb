{"cells":[{"cell_type":"markdown","metadata":{"id":"ZMR1meykK5_n"},"source":["# Lecture 8 - Time Series Analysis\n","\n","In this notebook we will learn how to perform time series analysis of Twitter data. \n","\n","<ol type = 1>\n","<li> Data processing</li>\n","    <ol type = a>\n","    <li> Load users' tweets</li>\n","    <li> Convert tweet times into datetime objects</li>\n","    <li> Shift times of tweets</li>\n","    </ol>\n","<li> Tweet rate</li>\n","    <ol type = a>\n","    <li> Calculate mean tweet rate</li>\n","    <li> Calculate rolling average tweet rate</li>\n","        <li> Word clouds of tweets in a specific window </li>\n","<li>Calculate rolling average of retweet count </li>\n","    </ol>\n"," \n","<li>Tweet hours and days</li>\n","\n","<ol type = a>\n","<li>Histograms of tweet hour and day of the week </li>\n","<li> Word clouds for tweets on a certain hour or day of the week </li>\n","</ol>\n","\n","<li> Estimate a user's time zone using high low model </li>\n","</ol>\n","</ol>\n","\n","This notebook can be opened in Colab \n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zlisto/social_media_analytics/blob/main/Lecture08_TimeSeriesAnalysis.ipynb)\n","\n","Before starting, select \"Runtime->Factory reset runtime\" to start with your directories and environment in the base state.\n","\n","If you want to save changes to the notebook, select \"File->Save a copy in Drive\" from the top menu in Colab.  This will save the notebook in your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"yjNwrmMjK5_u"},"source":["# Clones, installs, imports, and GPU\n"]},{"cell_type":"markdown","metadata":{"id":"JiogHhPAK5_u"},"source":["## Clone GitHub Repository\n","This will clone the repository to your machine.  This includes the code and data files.  Then change into the directory of the repository."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DDijd5QYK5_v","executionInfo":{"status":"ok","timestamp":1644824437760,"user_tz":300,"elapsed":5348,"user":{"displayName":"Tauhid Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidQnmUZhWqGKm0EkPJGymeiKs_-386XpfFQYQzQw=s64","userId":"02209064191632366608"}},"outputId":"80ac2cf7-d416-4e33-9236-d833ec5d71a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'social_media_analytics'...\n","remote: Enumerating objects: 172, done.\u001b[K\n","remote: Counting objects: 100% (172/172), done.\u001b[K\n","remote: Compressing objects: 100% (130/130), done.\u001b[K\n","remote: Total 172 (delta 93), reused 114 (delta 38), pack-reused 0\u001b[K\n","Receiving objects: 100% (172/172), 11.63 MiB | 6.48 MiB/s, done.\n","Resolving deltas: 100% (93/93), done.\n"]}],"source":["!git clone https://github.com/zlisto/social_media_analytics\n","\n","import os\n","os.chdir(\"social_media_analytics\")"]},{"cell_type":"markdown","metadata":{"id":"lD2V2v5VK5_w"},"source":["## Install Requirements "]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"IARUov6gK5_w","outputId":"8bbab840-9711-4ac8-aee2-51e5fb25dea1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.3.5)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.11.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.2.2)\n","Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.4.31)\n","Collecting loguru\n","  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 3.9 MB/s \n","\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (2.6.3)\n","Collecting transformers\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 19.9 MB/s \n","\u001b[?25hRequirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.5.0)\n","Collecting umap-learn\n","  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 2.8 MB/s \n","\u001b[?25hCollecting pyLDAvis==2.1.2\n","  Downloading pyLDAvis-2.1.2.tar.gz (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 28.7 MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 10)) (0.37.1)\n","Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 10)) (1.19.5)\n","Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 10)) (1.4.1)\n","Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 10)) (1.1.0)\n","Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 10)) (2.11.3)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 10)) (2.8.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 10)) (3.6.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 10)) (0.16.0)\n","Collecting funcy\n","  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2->-r requirements.txt (line 10)) (2.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 1)) (1.15.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->-r requirements.txt (line 4)) (4.10.1)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->-r requirements.txt (line 4)) (1.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 7)) (3.4.2)\n","Collecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n","\u001b[K     |████████████████████████████████| 6.8 MB 37.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 7)) (21.3)\n"]}],"source":["!pip install -r requirements.txt\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OeIfgObNK5_y"},"source":["## Import Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Owq0a2hsK5_y"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import datetime\n","\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","\n","import scripts.TextAnalysis as ta\n","from scripts.api import *"]},{"cell_type":"markdown","source":["# Data Processing"],"metadata":{"id":"XaILde4HK8GG"}},{"cell_type":"markdown","metadata":{"id":"gM-jbnznK5_z"},"source":["##  Load data\n","\n","Load the tweets in the file `\"data/tweets_lec_10.csv\"\"` into a dataframe. There are many users in this dataframe\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRC8J8HoK5_z"},"outputs":[],"source":["fname_db = \"data/lecture_04\"\n","df_all = DB.fetch(table_name = 'user_tweets', path = fname_db)\n","df_all.screen_name.unique()"]},{"cell_type":"markdown","metadata":{"id":"IHHDvvmoK5_0"},"source":["## Select Tweets of  a Screen Name and Keep Relevant Columns\n","\n","Choose a `screen_name` from those in `df_all`.  We will study this user's tweets.\n","\n","There are many columns in this dataframe, but we only need a few today.  We will keep only the columns `screen_name`, `created_at`, `text`, and `retweet_count`.  \n","\n","We will also be making word clouds, so apply the `clean_tweet` function to the `text` column and add this as a new column `text_clean` to the dataframe.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fmg2AfDAK5_0"},"outputs":[],"source":["\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QdcstgErK5_1"},"outputs":[],"source":["screen_name = 'kanyewest'\n","\n","df = df_all[df_all.screen_name==screen_name]\n","df = df[['screen_name','created_at','text','retweet_count']]\n","df['text_clean'] = df.text.apply(ta.clean_tweet)  #clean the tweets\n","\n","ntweets = len(df)\n","print(f\"dataframe has {ntweets} tweets\")\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"RfwgQ743K5_1"},"source":["## Datetimes\n","\n","Right now the tweet time is in the `created_at` column as a string.  To do time series analysis we have to convert it to a `datetime` object.  This can be done using the `to_datetime` function.  We have to specify the format of the date and time in the string.  We do this using the string variable `format`.  The `created_at` format is \"year-month-date hour:minute:second\".  In Python, this format is defined as `format = \"%Y-%m-%d %H:%M:%S\"`. \n","\n","We add a column `created_at_datetime` to the dataframe which are the `created_at` column converted to datetimes.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yI70KfKlK5_2"},"outputs":[],"source":["\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jne4X1DiK5_2"},"outputs":[],"source":["format = \"%Y-%m-%d %H:%M:%S\"\n","df['created_at_datetime'] = pd.to_datetime(df['created_at'],format=format).dt.tz_localize(None)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"-3EghFVIK5_2"},"source":["## Shifting Timezone\n","\n","The `created_at` field is in Coordinated Universal Time (UTC), which is the time in London.  If you want to shift the time to a different timezon, just add your local offset.\n","\n","For example, New York is -4 UTC, so we just add -4 hours to the UTC time.  This can be done using `timedelata64` object.  You specify you want hours with the string `'h'`, and the number of hours.  We will add this column to the dataframe and call it `created_at_datetime_ny`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mYw-000mK5_3"},"outputs":[],"source":["\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s900e92AK5_3"},"outputs":[],"source":["df['created_at_datetime_ny'] = df['created_at_datetime'] + np.timedelta64(-4, 'h')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"L1mfQSC3K5_3"},"source":["## Heads and Tails\n","\n","It's nice to sort the dataframe rows by the `created_at_datetime` column.  We can do this using the `sort_values` function.  Sort with `ascending=True` so that the first rows are the earliest tweets. Use the option `inplace=True` to have the dataframe be equal to its sorted version. \n","\n","To look at the first rows in the dataframe we use the `head` command.\n","\n","To look at the last rows in the dataframe we use the `tail` command."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCx80IT9K5_4"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjY_s7twK5_4"},"outputs":[],"source":["df.sort_values(by = 'created_at_datetime', inplace = True, ascending = True)\n","print(f\"Head:\\n{df.created_at.head()}\")\n","print(f\"Tail:\\n{df.created_at.tail()}\")\n","\n"]},{"cell_type":"markdown","source":["# Tweet Rate"],"metadata":{"id":"JTtCdlroLBsJ"}},{"cell_type":"markdown","metadata":{"id":"Fys_4m8NK5_4"},"source":["## Mean Tweet Rate\n","\n","To calculate the mean tweet rate, we find the number of tweets and divide it by the duration they span.  The number of tweets `ntweets` is just the number of rows in the dataframe, which you can obtain using the `len` function.  The duration of the tweets `duration` is found by subtracting the datetime of the earliest tweet from the datetime of the most recent tweets.  \n","\n","To obtain the earliest time we can use the `min` function.  To obtain the most recent time we can use the `max` function.\n","\n","`duration` is a `TimeDelta` object.  To make things simple, we convert it to a number of days using the `days` parameter.\n","\n","The `rate_estimate` is `ntweets` divided by `duration`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFTvtRkmK5_4"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9gGf_yGK5_4"},"outputs":[],"source":["ntweets = len(df)  #number of tweets\n","\n","duration = df.created_at_datetime.max()-df.created_at_datetime.min()  #duration in TimeDelta\n","duration = duration.days #duration in days\n","\n","rate_estimate = ntweets/duration  #rate [tweets/day]\n","\n","print('Rate estimate = %.2f tweets per day'%(rate_estimate))\n"]},{"cell_type":"markdown","metadata":{"id":"JXqhTCPJK5_5"},"source":["## Rolling Window Tweet Rate\n","\n","To obtain the rolling window tweet rate, we can make use of the `rolling` function.  This function creates a dataframe with rows grouped together if they are in a window specified by the row.  The `rolling` function is sort of like the `groupby` function we used before.\n","\n","We will specify the windows as being a fixed amount of time before the `created_at_datetime` column.  If we wanted a 1 day window, we use the format `rolling('1D',on='created_at_datetime'`.  If we wanted a 7 day window, we use the format `rolling('7D',on='created_at_datetime'`. After applying `rolling`, we need to apply another function to get the rate.  We want to know the number of tweets in the window.  To do this we create a column `tweet_indicator` that is 1 for every tweet.  Then to get the number of tweets in the window we apply`sum` after `rolling` to the `tweet_indicator` column.  \n","\n","The rate will be in units of tweets/day. If the window is 1 day, then after applying `sum` you have the rate in the proper units.  If the window is 7 days, you need to divide the `sum` by 7 to get the proper units.\n","\n","We add the rates to the dataframe as columns called `rate_1D` and `rate_7D`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0NDqiMEMK5_5"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RpTksPNhK5_5"},"outputs":[],"source":["df['tweet_indicator'] = np.ones(len(df))\n","df['rate_1D'] = (df.rolling('1D',on = 'created_at_datetime').sum()['tweet_indicator']-1)*1\n","df['rate_7D'] = (df.rolling('7D',on = 'created_at_datetime').sum()['tweet_indicator']-1)*1/7\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rdBrjIYIK5_5"},"source":["## Plot Rate\n","\n","The `lineplot` function allows us to plot the rate with different window sizes.  You can set the x-axis limits with the `xlim` command. The values are datetime objects, which are specified by `datetime.date(year,month,date)`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CC0V_47xK5_6"},"outputs":[],"source":["\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-DU0X0_XK5_6"},"outputs":[],"source":["fig = plt.figure(figsize = (12,6))\n","sns.lineplot(data = df, x = 'created_at_datetime',y = 'rate_1D',label = 'window = 1 day',linewidth = .25,color = 'blue')\n","sns.lineplot(data = df, x = 'created_at_datetime',y = 'rate_7D',label = 'window = 7 days', linewidth = 2,color = 'purple')\n","\n","plt.plot(df.created_at_datetime.tolist(),rate_estimate*np.ones(len(df)),label = 'mean rate',linestyle = '--',linewidth = 2,color = 'black')\n","\n","plt.grid()\n","plt.ylabel(\"Rate [tweets/day]\",fontsize = 16)\n","#plt.xlim(datetime.date(2021, 1, 1),datetime.date(2022,1,1))\n","#plt.ylim([0,30])\n","plt.title(f\"{screen_name}\",fontsize = 20)\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DsfY00AdK5_6"},"source":["## Tweets During Peak Tweet Rate\n","\n","Let's find the datetime when the tweet rate with a 1 day window is maximal and call this value `tpeak`.  We do this with the logical condition `df.rate_1D==df.rate_1D.max()`, and then getting the value in the `created_at_datetime` column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rstLPFBK5_6"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOMkt0jaK5_7"},"outputs":[],"source":["tpeak = df[df.rate_1D==df.rate_1D.max()].created_at_datetime.values[0]\n","print(f\"Peak tweet rate on {tpeak}\")\n"]},{"cell_type":"markdown","metadata":{"id":"s9umZQrMK5_7"},"source":["## Peak Rate Tweets\n","\n","We can make a dataframe of the tweets within a 24 hour window ending at `tpeak`.  We specify `t0` and `t1` as the start and stop stime for the window.  Then we use the logical condition  `(df.created_at_datetime>=t0) & (df.created_at_datetime<t1)` to get all tweets in this window and call the resulting dataframe `df1`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lE1EajimK5_7"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Yl_hW9lK5_7"},"outputs":[],"source":["t0 = tpeak - np.timedelta64(24, 'h')\n","t1 = tpeak \n","df1 = df[(df.created_at_datetime>=t0) & (df.created_at_datetime<t1)]\n","\n","df1.head()\n"]},{"cell_type":"markdown","metadata":{"id":"c3xaGdHaK5_8"},"source":["## Word Cloud of Peak Tweets\n","\n","We can make a wordcloud of the peak rate tweets to understand what was driving the increased activity that day."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTyqhxd8K5_8"},"outputs":[],"source":["stopwords = set(STOPWORDS)\n","text=' '.join(df1.text_clean.tolist()).lower()\n","wordcloud = WordCloud(stopwords=stopwords,max_font_size=150, max_words=100, background_color=\"white\",width=1000, height=600)\n","wordcloud.generate(text)\n","\n","fig = plt.figure(figsize = (6,6))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wAZCGVcpK5_8"},"source":["## Rolling Average of Retweet Count\n","\n","We can use the `rolling` function to plot rolling averages of numerical quantities like `retweet_count`.  We just apply the `mean` function.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BjnUMaHuK5_8"},"outputs":[],"source":["df['retweet_count_1D'] = df.rolling('1D',on = 'created_at_datetime').mean()['retweet_count']\n","df['retweet_count_7D'] = df.rolling('7D',on = 'created_at_datetime').mean()['retweet_count']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Ldcb2t6K5_8"},"outputs":[],"source":["fig = plt.figure(figsize = (12,6))\n","sns.lineplot(data = df, x = 'created_at_datetime',y = 'retweet_count_1D',label = 'window = 1 day',\n","             linewidth = .25,color = 'blue')\n","sns.lineplot(data = df, x = 'created_at_datetime',y = 'retweet_count_7D',label = 'window = 7 day',\n","             linewidth = 1,color = 'purple')\n","\n","\n","plt.grid()\n","plt.ylabel(\"Retweet count\",fontsize = 16)\n","#plt.xlim(datetime.date(2014, 1, 1),datetime.date(2021,1,1))\n","#plt.ylim([0,1e5])\n","plt.legend()\n","plt.title(f\"{screen_name}\",fontsize = 20)\n","plt.show()"]},{"cell_type":"markdown","source":["# Tweet Hours and Days"],"metadata":{"id":"VaiuqoNxLI5A"}},{"cell_type":"markdown","metadata":{"id":"9xotMp1WK5_9"},"source":["##  Histogram of Tweet Day of the Week\n","Create a column in the dataframe called `day` which has the day of the week of each tweet.  We get the day of the week of a tweet with the `dt.day_name()` function.  \n","\n","We can then make a histogram of the day of the user's tweets. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHw0Eqb-K5_9"},"outputs":[],"source":["df['day'] = df.created_at_datetime.dt.day_name()   #get the day of the tweet\n","\n","fig = plt.figure(figsize = (10,4))\n","sns.histplot(data = df, x = 'day')\n","plt.title(f\"{screen_name}\",fontsize = 20)\n","plt.ylabel(\"Tweet count\",fontsize = 14)\n","plt.xlabel(\"Day\",fontsize = 14)\n","plt.grid()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"lepDH5jYK5_9"},"source":["##  Tweet Hour Histograms\n","\n","Create a column `hour` with the hour of the tweets.  We can obtain the hour of a tweet with the `hour` parameter on the `datetime_created_at` column.  Note that the hours will be in UTC.\n","\n","Then we can make a histogram of the hour of each tweet for each user in the dataframe.  We will force the histogram to have have 24 bins going from 0 to 23 with the `bins` variable and the `bins` parameter in this `histplot` function.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbVQTzyZK5_9"},"outputs":[],"source":["df['hour'] = df.created_at_datetime.dt.hour   #get the day of the tweet\n","\n","bins = list(range(0,24))\n","\n","\n","fig = plt.figure(figsize = (10,4))\n","sns.histplot(data = df, x = 'hour',bins = bins)\n","plt.title(f\"{screen_name}\",fontsize = 20)\n","plt.ylabel(\"Tweet count\",fontsize = 14)\n","plt.xlabel(\"Hour [UTC]\",fontsize = 14)\n","plt.grid()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"uKXZDcjzK5_-"},"source":["## Tweets During a Specific Hour or Day\n","\n","We can get the tweets posted during a specific hour (i.e. 10 AM) or day of the week (i.e. Monday) using simple logical conditions applied to the `day` and `hour` columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYr1eshAK5_-"},"outputs":[],"source":["df_hour = df[df.hour==10]\n","df_hour.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"CXqYsqJ6K5_-"},"outputs":[],"source":["day = 'Wednesday'\n","df_day = df[df.day==day]\n","df_day.head()"]},{"cell_type":"markdown","metadata":{"id":"nL98UqxCK5_-"},"source":["## Word Cloud of Tweets on a Specific Hour or Day\n","\n","Once you create a dataframe with tweets from a specific time period, hour, or day, you can easily make a word cloud of them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUPmi-OAK5__"},"outputs":[],"source":["stopwords = set(STOPWORDS)\n","stopwords.add(screen_name)\n","text=' '.join(df_day.text_clean.tolist()).lower()\n","wordcloud = WordCloud(stopwords=stopwords,max_font_size=150, max_words=100, background_color=\"white\",width=1000, height=600)\n","wordcloud.generate(text)\n","\n","fig = plt.figure(figsize = (6,6))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.title(f\"Tweets of {screen_name} on {day}\",fontsize = 20)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"2E3s9RQHK5__"},"source":["# Geo-location with Tweet Times \n","\n","Now we will try to figure out what time zone people are based on when their Twitter activity is low."]},{"cell_type":"markdown","metadata":{"id":"pBDy8rwDK5__"},"source":["##  Tweet Hour Histograms\n","\n","Let's begin by making a histogram of the hour of each tweet for each user in the dataframe. We will specify the number of bins, as we did before.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"obx0V29kK5__"},"outputs":[],"source":["bins = list(range(0,24))\n","\n","for screen_name in df.screen_name.unique():\n","    df1 = df[df.screen_name==screen_name]\n","    fig = plt.figure(figsize = (10,4))\n","    sns.histplot(data = df1, x = 'hour',bins = bins)\n","    plt.title(f\"{screen_name}\",fontsize = 20)\n","\n","    plt.ylabel(\"Tweet count\",fontsize = 14)\n","    plt.xlabel(\"Hour\",fontsize = 14)\n","    plt.grid()\n","    plt.show()  "]},{"cell_type":"markdown","metadata":{"id":"VQdZ6TdLK5__"},"source":["## Function to Find UTC Offset based on High Low Model\n","\n","The function `geolocate_utc_offset` takes as input a dataframe of tweets `df` and a list of hours when activity is low, `low_hours` and returns the UTC offset that puts the fewest number of tweets in the low activity period.  You need a column `hour` in the dataframe that has the hour of each tweet.  \n","\n","`geolocate_utc_offset` returns three things:\n","\n","1. `utc_offset_min` = UTC offset to put fewest tweets in `low_hours`\n","\n","2. `nlows` = list of number of tweets during `low_hours` for each possible UTC offset\n","\n","3. `utc_offsets` = list of possible UTC offsets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0K42tOWK6AA"},"outputs":[],"source":["def geolocate_utc_offset(df,low_hours):\n","    nlows = []\n","    utc_offsets = list(range(-11,13))\n","    for utc_offset in utc_offsets:\n","        low_hours_offset = [(h-utc_offset)%24 for h in low_hours]\n","        nlow = len(df1[df1.hour.isin(low_hours_offset)])\n","        nlows.append(nlow)\n","    utc_offset_min = utc_offsets[nlows.index(min(nlows))]\n","    return utc_offset_min,nlows,utc_offsets"]},{"cell_type":"markdown","metadata":{"id":"bPz8sW84K6AA"},"source":["## Find UTC Offset\n","\n","We specify `low_hours` as a list of hours when we think activity is low.  Then a `for` loop goes through each screen name and finds their UTC offset.  This `for` loop might come in handy on your homework :)\n","\n","To see possible cities in the time zone corresponding to the UTC offset, use this link:\n","\n","https://www.timeanddate.com/time/map/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtD1y-2pK6AA"},"outputs":[],"source":["low_hours = [4,5]  #hours when activity is low in local time zone \n","for screen_name in df.screen_name.unique():\n","    df1 = df[df.screen_name==screen_name]\n","    utc_offset_min,nlows,utc_offsets = geolocate_utc_offset(df1,low_hours)\n","    print(f\"{screen_name}: UTC offset = {utc_offset_min}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNeS0Yc7K6AA"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNztnH2ZK6AA"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"colab":{"name":"Lecture08_TimeSeriesAnalysis.ipynb","provenance":[],"collapsed_sections":["-3EghFVIK5_2","JXqhTCPJK5_5","rdBrjIYIK5_5","DsfY00AdK5_6","s9umZQrMK5_7","lepDH5jYK5_9","uKXZDcjzK5_-","pBDy8rwDK5__","VQdZ6TdLK5__","bPz8sW84K6AA"],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}